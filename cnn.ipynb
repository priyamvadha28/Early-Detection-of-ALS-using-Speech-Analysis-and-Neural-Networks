{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_dir = r\"C:\\Users\\Priyamvadha Pradeep\\Desktop\\FYP\\CNN\\New Models\\train\"\n",
    "val_dir = r\"C:\\Users\\Priyamvadha Pradeep\\Desktop\\FYP\\CNN\\New Models\\validate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image parameters\n",
    "img_width, img_height = 128, 128\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 978 images belonging to 2 classes.\n",
      "Found 246 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data with flow_from_directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to address imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Priyamvadha Pradeep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),  # Adjust input shape to match spectrogram size\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),  # Reduced size of dense layer to avoid overfitting\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2, activation='softmax')  # Adjust for number of classes (2 classes for binary classification)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 978 files belonging to 2 classes.\n",
      "Found 246 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the corrected paths using raw string literals\n",
    "train_dir = r'C:\\Users\\Priyamvadha Pradeep\\Desktop\\FYP\\CNN\\New Models\\train'\n",
    "val_dir = r'C:\\Users\\Priyamvadha Pradeep\\Desktop\\FYP\\CNN\\New Models\\validate'\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 545ms/step - accuracy: 0.5610 - loss: 5.7964 - val_accuracy: 0.3821 - val_loss: 0.8503 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 513ms/step - accuracy: 0.6658 - loss: 0.6581 - val_accuracy: 0.6179 - val_loss: 0.6726 - learning_rate: 9.5000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 536ms/step - accuracy: 0.6756 - loss: 0.6332 - val_accuracy: 0.6382 - val_loss: 0.6739 - learning_rate: 9.0250e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 543ms/step - accuracy: 0.6522 - loss: 0.6233 - val_accuracy: 0.6626 - val_loss: 0.6704 - learning_rate: 8.5737e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 572ms/step - accuracy: 0.6620 - loss: 0.6022 - val_accuracy: 0.6667 - val_loss: 0.6770 - learning_rate: 8.1451e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 553ms/step - accuracy: 0.6662 - loss: 0.6097 - val_accuracy: 0.6423 - val_loss: 0.7005 - learning_rate: 7.7378e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 537ms/step - accuracy: 0.6490 - loss: 0.5951 - val_accuracy: 0.6423 - val_loss: 0.6899 - learning_rate: 7.3509e-04\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up learning rate scheduler and early stopping callbacks\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 0.95 ** epoch)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with reduced epochs\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,  # Start with 20 epochs\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[lr_schedule, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('cnn_als_classifier.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.6662 - loss: 0.6752\n",
      "Validation Loss: 0.6703668236732483\n",
      "Validation Accuracy: 0.6626016497612\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
